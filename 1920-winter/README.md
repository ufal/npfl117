# Deep Learning Seminar, Winter 2019/20

## Tab: Home

In recent years, **deep neural networks** have been used to solve complex
machine-learning problems and have achieved significant **state-of-the-art**
results in **many** areas. The whole field of deep learning has been developing
**rapidly**, with **new methods** and **techniques** emerging steadily.

The goal of the seminar is to follow the **newest advancements** in the deep
learning field. The course takes form of a **reading group** – each lecture
a paper is presented by one of the students. The paper is announced in advance,
hence all participants can read it beforehand and can take part in the
**discussion of the paper**.

If you want to receive announcements about chosen paper, sign up to our
[mailing list ufal-rg@googlegroups.com](https://groups.google.com/forum/#!forum/ufal-rg).

### About

SIS code: [NPFL117](https://is.cuni.cz/studium/eng/predmety/index.php?do=predmet&kod=NPFL117)<br>
Semester: winter + summer<br>
E-credits: 3<br>
Examination: 0/2 C<br>
Guarantor: [Milan Straka](https://ufal.mff.cuni.cz/milan-straka)

### Timespace Coordinates

The Deep Learning Seminar takes place on **Monday** at **12:20** in **S10**. We will first meet on Monday **Oct 07**.

### Requirements

To pass the course, you need to **present a research paper** and sufficiently
attend the presentations.

### License

Unless otherwise stated, teaching materials for this course are available under
[CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/).

## Tab: Program

If you want to receive announcements about chosen paper, sign up to our
[mailing list ufal-rg@googlegroups.com](https://groups.google.com/forum/#!forum/ufal-rg).

To add your name to a paper the table below, edit the
[source code on GitHub](https://github.com/ufal/npfl117/edit/master/1920-winter/README.md) and send a PR.

<div class="program"><style>
  .program td { vertical-align: middle !important}
  .program tr>td:nth-of-type(1), .program tr>td:nth-of-type(2), .program tr>td:nth-of-type(3) {white-space: nowrap}
</style>

| Date        | Who                | Topic | Paper(s)
| ----        | ---                | ----- | --------
| 07 Oct 2019 | Milan Straka       | Transformer | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)**<br> Peter Shaw, Jakob Uszkoreit, Ashish Vaswani: **[Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)**<br> Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, Douglas Eck: **[Music Transformer](https://arxiv.org/abs/1809.04281)**<br> Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov: **[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)** |
| 14 Oct 2019 | Ondřej Měkota      | Transformer | Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. **[BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)**<br> Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le. **[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)**
| 21 Oct 2019 | Tomas Soucek       | 3D Pointclouds | Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas: **[PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413)**<br> Christopher Choy, JunYoung Gwak, Silvio Savarese: **[4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks](https://arxiv.org/abs/1904.08755)**<br> Christopher Choy, Jaesik Park, Vladlen Koltun: **[Fully Convolutional Geometric Features](https://node1.chrischoy.org/data/publications/fcgf/fcgf.pdf)**
| 28 Oct 2019 |*No DL seminar*     |       |*Czech Independence Day*
| 04 Nov 2019 | Zdeněk Kasner | Neural LMs  | Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. **[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)** ([OpenAI blog post](https://openai.com/blog/better-language-models/))<br> Subramanian, Sandeep, Raymond Li, Jonathan Pilault, and Christopher Pal. **[On Extractive and Abstractive Neural Document Summarization with Transformer Language Models](https://arxiv.org/pdf/1909.03186.pdf)** |
| 11 Nov 2019 - 45 min | Viktor Vašátko     | Adversarial Examples | Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li: **[Adversarial Examples: Attacks and Defenses for Deep Learning](https://arxiv.org/abs/1712.07107)** |
| 11 Nov 2019 - 45 min | Jan Vainer         | Normalizing flows, Real NVP, Glow    | Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio: **[Density estimation using Real NVP](https://arxiv.org/abs/1605.08803)**<br>Diederik P. Kingma, Prafulla Dhariwal: **[Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/abs/1807.03039)**
| 18 Nov 2019 | Memduh Gokirmak<br> Abhishek Agrawal| NN Interpretation        | Hewitt, John, and Christopher D. Manning. **[A Structural Probe for Finding Syntax in Word Representations](https://nlp.stanford.edu/pubs/hewitt2019structural.pdf)** <br> Ning Mei, Usman Sheikh, Roberto Santana and David Soto **[How the brain encodes meaning: Comparing word embedding and computer vision models to predict fMRI data during visual word recognition](https://ccneuro.org/2019/proceedings/0000863.pdf)**
| 25 Nov 2019 | Erdi Düzel   | Image Segmentation  |Xiaomei Zhao, Yihong Wu, Guidong Song, Zhenye Li, Yazhuo Zhang, Yong Fan **[A deep learning model integrating FCNNs and CRFs for brain tumor segmentation](https://www.sciencedirect.com/science/article/pii/S136184151730141X)**
| 02 Dec 2019 | Milan Straka | Optimization | Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, Cho-Jui Hsieh: **[Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)**<br> Michael R. Zhang, James Lucas, Geoffrey Hinton, Jimmy Ba: **[Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610)**<br> Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han: **[On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265)**
| 09 Dec 2019 | Václav Volhejn     | Overfitting and generalization |  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals. **[Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)**<br>Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. **[Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/abs/1812.11118)**<br>Hartmut Maennel, Olivier Bousquet, Sylvain Gelly. **[Gradient Descent Quantizes ReLU Network Features](https://arxiv.org/abs/1803.08367)**      |
| 16 Dec 2019 | David Samuel       | Neural ODEs    | Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud. **[Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)**<br>Emilien Dupont, Arnaud Doucet, Yee Whye Teh. **[Augmented Neural ODEs](https://arxiv.org/abs/1904.01681)**<br>Yulia Rubanova, Ricky T. Q. Chen, David Duvenaud. **[Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907)**
|*23 Dec 2019*|*No DL seminar*     |       | *Christmas Holiday*
|*30 Dec 2019*|*No DL seminar*     |       | *Christmas Holiday*
| 06 Jan 2020 | David Kubeša       | Entity Linking | Nikolaos Kolitsas, Octavian-Eugen Ganea, Thomas Hofmann. **[End-to-End Neural Entity Linking](https://arxiv.org/abs/1808.07699)**<br>Possibly also Samuel Broscheit. **[Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking](https://www.aclweb.org/anthology/K19-1063.pdf)**

</div>

## Tab: Papers

You can choose **any paper** you find interesting, but if you would like some inspiration, you can look at the following list.
The papers are grouped, each group is expected to be presented on one seminar.

#### Natural Language Processing

- &nbsp;
  - ~~Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin: **[Advances in Pre-Training Distributed Word Representations](https://arxiv.org/abs/1712.09405)**~~
  - ~~Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer: **[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)**~~
  - ~~Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: **[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)**~~
  - ~~Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le. **[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)**~~


#### Generative Modeling

- &nbsp;
  - Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio: **[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)**
  - Martin Arjovsky, Soumith Chintala, Léon Bottou: **[Wasserstein GAN](https://arxiv.org/abs/1701.07875)**

- &nbsp;
  - Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen: **[Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196)**
  - Andrew Brock, Jeff Donahue, Karen Simonyan: **[Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096)**
  - Tero Karras, Samuli Laine, Timo Aila: **[A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948)**

- &nbsp;
  - ~~Laurent Dinh, David Krueger, Yoshua Bengio: **[NICE: Non-linear Independent Components Estimation](https://arxiv.org/abs/1410.8516)**~~
  - ~~Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio: **[Density estimation using Real NVP](https://arxiv.org/abs/1605.08803)**~~
  - ~~Diederik P. Kingma, Prafulla Dhariwal: **[Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/abs/1807.03039)**~~

#### Neural Architecture Search (AutoML)

- &nbsp;
  - Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le: **[Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012)**
  - Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le: **[Regularized Evolution for Image Classifier Architecture Search](https://arxiv.org/abs/1802.01548)**
  - Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy: **[Progressive Neural Architecture Search](https://arxiv.org/abs/1712.00559)**

- &nbsp;
  - Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean: **[Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/abs/1802.03268)**
  - Hanxiao Liu, Karen Simonyan, Yiming Yang: **[DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)**

#### Networks with External Memory

- Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap: **[One-shot Learning with Memory-Augmented Neural Networks](https://arxiv.org/abs/1605.06065)**

- Mark Collier, Joeran Beel: **[Memory-Augmented Neural Networks for Machine Translation](https://arxiv.org/abs/1909.08314)**

#### Optimization
- &nbsp;
  - ~~Michael R. Zhang, James Lucas, Geoffrey Hinton, Jimmy Ba: **[Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610)**~~
  - ~~Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han: **[On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265)**~~

#### Adversarial Examples
- &nbsp;
  - ~~Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li: **[Adversarial Examples: Attacks and Defenses for Deep Learning](https://arxiv.org/abs/1712.07107)**~~
  - Jiliang Zhang, Chen Li: **[Adversarial Examples: Opportunities and Challenges](https://arxiv.org/abs/1809.04790)**

## Tab: Related Courses

### Related Coursed

#### [Deep Learning](https://ufal.mff.cuni.cz/courses/npfl114)
Course introducing deep neural networks, from the basics to the latest advances,
focusing both on **theory** as well as on **practical aspects**.

#### [Deep Reinforcement Learning](https://ufal.mff.cuni.cz/courses/npfl122)
Course introducing reinforcement learning, from basic tabular methods to
involvement of deep neural networks, focusing both on **theory** as well as on
**practical aspects**.

#### [Machine Learning for Greenhorns](https://ufal.mff.cuni.cz/courses/npfl129)
Introductory course to machine learning, focusing both on **theoretical foundations**
as well as on **practical applications** in **Python**.

#### [ÚFAL Reading Group](https://ufal.mff.cuni.cz/courses/rg)
Previously, Deep Learning Seminar was held unofficially as ÚFAL Reading Group,
you can see the discussed paper here:
- [ÚFAL Reading Group 2016](https://ufal.mff.cuni.cz/courses/rg/2016)
- [ÚFAL Reading Group 2015](https://ufal.mff.cuni.cz/courses/rg/2015)
- [ÚFAL Reading Group 2014](https://ufal.mff.cuni.cz/courses/rg/2014)

## Tab: Archive

### Archive

#### [Deep Learning Seminar, Summer 2018/19](https://ufal.mff.cuni.cz/courses/npfl117/1819-summer)

#### [Deep Learning Seminar, Winter 2018/19](https://ufal.mff.cuni.cz/courses/npfl117/1819-winter)

#### [Deep Learning Seminar, Summer 2017/18](https://ufal.mff.cuni.cz/courses/npfl117/1718-summer)

#### [Deep Learning Seminar, Summer 2016/17](https://ufal.mff.cuni.cz/courses/npfl117/1617-summer)
